{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Wiki_Scraping.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1IDT044UqOItsDE3oBsDjsijjjRLhQGdS",
      "authorship_tag": "ABX9TyOrJvZ7sIDRA+dqDNRA3oen",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kartik727/ml-projects/blob/master/wiki_scrape/Wiki_Scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sskG7q-2JaDh"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup as bsp\n",
        "from queue import Queue\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "from urllib.parse import unquote"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    'loc' : {\n",
        "        'base_dir' : '/content/drive/MyDrive/ML/Data/wiki_scrape/',\n",
        "        'title_file' : 'names.txt',\n",
        "        'data_dir' : 'articles/',\n",
        "        'data_ext' : '.txt'\n",
        "    },\n",
        "    'base_url' : 'https://en.wikipedia.org',\n",
        "    'starting_link' : '/wiki/Hypernova',\n",
        "    'num_articles' : 100\n",
        "}"
      ],
      "metadata": {
        "id": "AX0plqlxw1p2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = '/content/drive/MyDrive/ML/Data/wiki_scrape/'\n",
        "articles = base_dir+config['loc']['data_dir']"
      ],
      "metadata": {
        "id": "5Nc0VuOkRdeO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean the working directory\n",
        "! rm {articles}*"
      ],
      "metadata": {
        "id": "sfa2KHaNRC62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d4feade-1369-485a-bf1c-e331f89ea17f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '/content/drive/MyDrive/ML/Data/wiki_scrape/articles/*': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Wiki_Article:\n",
        "    \"\"\"\n",
        "    Representation of a wikipedia article. Stores the relative link of an article and creates a Beautiful Soup object\n",
        "    using the url\n",
        "\n",
        "    ...\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    url : str\n",
        "        Relative url of the article\n",
        "\n",
        "    Methods\n",
        "    -------\n",
        "    parse_data:\n",
        "        Parses the paragraphs of the article into list of strings\n",
        "\n",
        "    parse_links:\n",
        "        Parses all hyperlinks to other wikipedia articles into list of strings\n",
        "\n",
        "    get_paragraphs:\n",
        "        Returns the paragraphs (primary text) of the article\n",
        "\n",
        "    get_wiki_links:\n",
        "        Returns a list of all the links to other wikipedia articles in the current artcle\n",
        "    \"\"\"\n",
        "\n",
        "    BASE_URL=config['base_url']\n",
        "\n",
        "    def __init__(self, url:str):\n",
        "        self.url = url\n",
        "        response = requests.get(url=self.BASE_URL+url)\n",
        "        self.soup = bsp(response.content, 'html.parser')\n",
        "        self.paragraphs = None\n",
        "        self.links = None\n",
        "        \n",
        "        # Setting the title of the article\n",
        "        title = self.soup.find(id='firstHeading')\n",
        "        self.title = title.string\n",
        "\n",
        "    def parse_data(self, add_title:bool=False)->None:\n",
        "        '''Parses the paragraphs of the article into list of strings'''\n",
        "\n",
        "        data = self.soup.find(id='mw-content-text')\n",
        "        data = data.find(class_='mw-parser-output')\n",
        "        paragraph_list = []\n",
        "\n",
        "        if add_title:\n",
        "            paragraph_list.append(self.title)\n",
        "\n",
        "        for para in data.find_all('p'):\n",
        "            paragraph_list.append(para.get_text())\n",
        "\n",
        "        self.paragraphs = '\\n'.join(paragraph_list)\n",
        "\n",
        "    def parse_links(self)->None:\n",
        "        '''Parses all hyperlinks to other wikipedia articles into list of strings'''\n",
        "\n",
        "        self.links = []\n",
        "        all_links = self.soup.find(id='bodyContent').find_all('a')\n",
        "        for link in all_links:\n",
        "            try:\n",
        "                txt = link['href']\n",
        "\n",
        "                # Don't want non-wikipedia links\n",
        "                if txt[:6] != '/wiki/':\n",
        "                    continue\n",
        "                \n",
        "                # Don't want special pages\n",
        "                if ':' in txt:\n",
        "                    continue\n",
        "\n",
        "                # Remove any shortcuts\n",
        "                txt = txt.split('#')[0]\n",
        "\n",
        "                # Remove disambiguation links\n",
        "                if txt[-17:] == '_(disambiguation)':\n",
        "                    continue\n",
        "\n",
        "                # Add to the list of links\n",
        "                self.links.append(txt)\n",
        "            except KeyError:\n",
        "                pass                \n",
        "\n",
        "    def get_paragraphs(self, add_title:bool=False):\n",
        "        '''Returns the paragraphs (primary text) of the article'''\n",
        "        if self.paragraphs is None:\n",
        "            self.parse_data(add_title=add_title)\n",
        "\n",
        "        return self.paragraphs\n",
        "\n",
        "    def get_wiki_links(self):\n",
        "        '''Returns a list of all the links to other wikipedia articles in the current artcle'''\n",
        "        if self.links is None:\n",
        "            self.parse_links()\n",
        "\n",
        "        return self.links"
      ],
      "metadata": {
        "id": "gYEPTMTXTYy1"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_wiki_articles(articles:set, start_link:str, num_articles:int, base_dir:str, ext:str)->None:\n",
        "    \"\"\"\n",
        "    Scrapes the requested number of articles from wikipedia that are not already present in the given set of articles\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    articles (set):\n",
        "        The set of articles that have already been scraped and should be avoided\n",
        "\n",
        "    start_link (str):\n",
        "        The link to the starting article from which new links will be recursively generated\n",
        "\n",
        "    num_artices (int):\n",
        "        Number of articles to be scraped\n",
        "\n",
        "    base_dir (int):\n",
        "        Directory in which the results will be stored\n",
        "\n",
        "    ext (str):\n",
        "        Extension of the files containing the scraped articles\n",
        "    \"\"\"\n",
        "\n",
        "    target_len = len(articles) + num_articles\n",
        "    new_links = set()\n",
        "    new_links.add(start_link)\n",
        "\n",
        "    \n",
        "    while len(articles) < target_len:\n",
        "\n",
        "        # Stop if ran out of articles\n",
        "        if len(new_links)==0:\n",
        "            print('Article links exhausted')\n",
        "            break\n",
        "\n",
        "        # Select a random article and remove from the set\n",
        "        link = random.choice(list(new_links))\n",
        "        new_links.remove(link)\n",
        "        created = False\n",
        "\n",
        "        # Read seen articles to get more links if links set getting small\n",
        "        if len(new_links) < 100:\n",
        "            article = Wiki_Article(link)\n",
        "            created = True\n",
        "            for l in article.get_wiki_links():\n",
        "                new_links.add(l)\n",
        "\n",
        "        # Parse unseen articles and add their links to the set\n",
        "        if link not in articles:\n",
        "            if not created:\n",
        "                article = Wiki_Article(link)\n",
        "                for l in article.get_wiki_links():\n",
        "                    new_links.add(l)\n",
        "            title = article.title\n",
        "            if title is not None:\n",
        "                articles.add(link)\n",
        "                title_unq = unquote(link.split('/')[-1])\n",
        "                data = article.get_paragraphs(add_title=True)\n",
        "                with open(base_dir+title_unq+ext, 'w') as f:\n",
        "                    f.write(data)\n",
        "\n",
        "    print('Scraping complete')\n"
      ],
      "metadata": {
        "id": "UIpvcAFdXG8K"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loc = config['loc']\n",
        "\n",
        "# Reading the file with all article links\n",
        "try:\n",
        "    with open(loc['base_dir']+loc['title_file'], 'r') as f:\n",
        "        names = set(f.read().split('\\n'))\n",
        "        starting_link = random.sample(list(names), 1)[0]\n",
        "        print('Title file loaded.')\n",
        "except FileNotFoundError:\n",
        "    names = set()\n",
        "    starting_link = config['starting_link']\n",
        "    print('Title file not found. Starting from scratch.')\n",
        "\n",
        "print(f'First link: {starting_link}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZRTuqc2qVoG",
        "outputId": "824f6eea-7dcf-40b2-ac2c-39156dc559d2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title file loaded.\n",
            "First link: /wiki/Pressure_cooker\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Scraping Wikipedia for new articles\n",
        "print(f'Number of articles before batch: {len(names)}')\n",
        "\n",
        "scrape_wiki_articles(names, starting_link, config['num_articles'], loc['base_dir']+loc['data_dir'], loc['data_ext'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYm2-Ovb-nh5",
        "outputId": "a8e46a18-6a9a-4845-95d0-c28159edd9b9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of articles before batch: 101\n",
            "Scraping complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Writing the title names (including new ones) to disk\n",
        "with open(loc['base_dir']+loc['title_file'], 'w') as f:\n",
        "    for name in names:\n",
        "        f.write(name + '\\n')\n",
        "    print('Written links to disk')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyJzvH2KDfUx",
        "outputId": "6d56f9d8-b8e2-4ed8-a7b7-3f614c427a68"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Written links to disk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "B0ID7XH8D8KA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}